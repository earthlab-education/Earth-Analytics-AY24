{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8aede979-068f-489d-a683-139e58cb6013",
      "metadata": {},
      "source": [
        "# Land cover classification at the Mississppi Delta\n",
        "\n",
        "In this notebook, you will use a k-means **unsupervised** clustering\n",
        "algorithm to group pixels by similar spectral signatures. **k-means** is\n",
        "an **exploratory** method for finding patterns in data. Because it is\n",
        "unsupervised, you don’t need any training data for the model. You also\n",
        "can’t measure how well it “performs” because the clusters will not\n",
        "correspond to any particular land cover class. However, we expect at\n",
        "least some of the clusters to be identifiable as different types of land\n",
        "cover.\n",
        "\n",
        "You will use the [harmonized Sentinal/Landsat multispectral\n",
        "dataset](https://lpdaac.usgs.gov/documents/1698/HLS_User_Guide_V2.pdf).\n",
        "You can access the data with an [Earthdata\n",
        "account](https://www.earthdata.nasa.gov/learn/get-started) and the\n",
        "[`earthaccess` library from\n",
        "NSIDC](https://github.com/nsidc/earthaccess):\n",
        "\n",
        "### STEP 1: SET UP"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edde10bd",
      "metadata": {},
      "source": [
        "**BLOCK 0**<br>\n",
        "Import all libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3f0f55c",
      "metadata": {
        "highlight": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import re\n",
        "import warnings\n",
        "import json \n",
        "\n",
        "import cartopy.crs as ccrs\n",
        "import earthaccess\n",
        "import earthpy as et\n",
        "import geopandas as gpd\n",
        "import geoviews as gv\n",
        "import hvplot.pandas\n",
        "import hvplot.xarray\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import rioxarray as rxr\n",
        "import rioxarray.merge as rxrmerge\n",
        "from tqdm.notebook import tqdm\n",
        "import xarray as xr\n",
        "from shapely.geometry import Polygon\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "os.environ[\"GDAL_HTTP_MAX_RETRY\"] = \"5\"\n",
        "os.environ[\"GDAL_HTTP_RETRY_DELAY\"] = \"1\"\n",
        "\n",
        "warnings.simplefilter('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00d1daa3-a10e-472b-94ff-4ff2db336bf5",
      "metadata": {},
      "source": [
        "Below you can find code for a caching **decorator** which you can use in\n",
        "your code. To use the decorator:\n",
        "\n",
        "``` python\n",
        "@cached(key, override)\n",
        "def do_something(*args, **kwargs):\n",
        "    ...\n",
        "    return item_to_cache\n",
        "```\n",
        "\n",
        "This decorator will **pickle** the results of running the\n",
        "`do_something()` function, and only run the code if the results don’t\n",
        "already exist. To override the caching, for example temporarily after\n",
        "making changes to your code, set `override=True`. Note that to use the\n",
        "caching decorator, you must write your own function to perform each\n",
        "task!\n",
        "\n",
        "Notes: \n",
        "If the cached file (filename) exists and override is False, the function loads the results from the pickle file.\n",
        "If the file doesn’t exist or override=True, the function runs normally, and the result is saved in a pickle file.\n",
        "This avoids redundant computations, especially for expensive functions.\n",
        "\n",
        "In Python, **do_something()** is simply a generic function name that is commonly used as an example in documentation and tutorials to represent a function that performs some specific task. It is not a built-in Python function, but a placeholder for any function you define.\n",
        "For example, you could define do_something() to perform a mathematical operation, process data, access an API, read a file, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b16eb55",
      "metadata": {},
      "source": [
        "**BLOCK 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad0a9842",
      "metadata": {},
      "outputs": [],
      "source": [
        "def cached(func_key, override=False):\n",
        "    \"\"\"\n",
        "    A decorator to cache function results\n",
        "    \n",
        "    Parameters\n",
        "    ==========\n",
        "    key: str\n",
        "      File basename used to save pickled results\n",
        "    override: bool\n",
        "      When True, re-compute even if the results are already stored\n",
        "    \"\"\"\n",
        "    def compute_and_cache_decorator(compute_function): \n",
        "        \"\"\"\n",
        "        Wrap the caching function\n",
        "        \n",
        "        Parameters\n",
        "        ==========\n",
        "        compute_function: function\n",
        "          The function to run and cache results\n",
        "        \"\"\"\n",
        "        def compute_and_cache(*args, **kwargs): \n",
        "            \"\"\"\n",
        "            Perform a computation and cache, or load cached result.\n",
        "            \n",
        "            Parameters\n",
        "            ==========\n",
        "            args\n",
        "              Positional arguments for the compute function\n",
        "            kwargs\n",
        "              Keyword arguments for the compute function\n",
        "            \"\"\"\n",
        "            # Add an identifier from the particular function call \n",
        "            if 'cache_key' in kwargs:\n",
        "                key = '_'.join((func_key, kwargs['cache_key']))\n",
        "            else:\n",
        "                key = func_key  \n",
        "\n",
        "            path = os.path.join(et.io.HOME, et.io.DATA_NAME, 'jars', f'{key}.pickle') \n",
        "                       \n",
        "            # Check if the cache exists already or override caching \n",
        "            if override or not os.path.exists(path):  \n",
        "                # Make jars directory if needed\n",
        "                os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "                \n",
        "                # Run the compute function as the user did \n",
        "                result = compute_function(*args, **kwargs)\n",
        "                \n",
        "                # Pickle the object \n",
        "                with open(path, 'wb') as file:\n",
        "                    pickle.dump(result, file)\n",
        "            else:\n",
        "                # Unpickle the object \n",
        "                with open(path, 'rb') as file:\n",
        "                    result = pickle.load(file)\n",
        "                    \n",
        "            return result \n",
        "        \n",
        "        return compute_and_cache \n",
        "    return compute_and_cache_decorator"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0c7ef24",
      "metadata": {},
      "source": [
        "This code defines a Python cache decorator cached(), which stores the results of a \n",
        "function in a pickle file and reuses them in future executions to avoid \n",
        "unnecessary recomputations.\n",
        "\n",
        "What does this code do?\n",
        "Saves the results of a function in a **.pickle file**, avoiding repeated computations.\n",
        "\n",
        "It uses a key identifier (func_key) to generate the name of the file where the cache will be saved.\n",
        "\n",
        "If override=True, reruns the function and updates the cache.\n",
        "\n",
        "Stores the files in a specific directory (et.io.HOME/et.io.DATA_NAME/jars/).\n",
        "\n",
        "If the cache exists, it loads the results from the file instead of recalculating."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f8cf346-8ef9-4c74-8835-411b1815aac2",
      "metadata": {},
      "source": [
        "### STEP 2: STUDY SITE\n",
        "\n",
        "For this analysis, you will use a watershed from the [Water Boundary\n",
        "Dataset](https://www.usgs.gov/national-hydrography/access-national-hydrography-products),\n",
        "HU12 watersheds (WBDHU12.shp).\n",
        "\n",
        "<link rel=\"stylesheet\" type=\"text/css\" href=\"./assets/styles.css\"><div class=\"callout callout-style-default callout-titled callout-task\"><div class=\"callout-header\"><div class=\"callout-icon-container\"><i class=\"callout-icon\"></i></div><div class=\"callout-title-container flex-fill\">Try It</div></div><div class=\"callout-body-container callout-body\"><ol type=\"1\">\n",
        "<li>Download the Water Boundary Dataset for region 8 (Mississippi)</li>\n",
        "<li>Select watershed 080902030506</li>\n",
        "<li>Generate a site map of the watershed</li>\n",
        "</ol>\n",
        "<p>Try to use the <strong>caching decorator</strong></p></div></div>\n",
        "\n",
        "We chose this watershed because it covers parts of New Orleans an is\n",
        "near the Mississippi Delta. Deltas are boundary areas between the land\n",
        "and the ocean, and as a result tend to contain a rich variety of\n",
        "different land cover and land use types."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a38841af",
      "metadata": {},
      "source": [
        "**BLOCK 2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ccaa14b",
      "metadata": {},
      "outputs": [],
      "source": [
        "@cached('wbd_08')                                                    \n",
        "def read_wbd_file(wbd_filename, huc_level, cache_key): \n",
        "    # Download and unzip\n",
        "    wbd_url = (\n",
        "        \"https://prd-tnm.s3.amazonaws.com\"\n",
        "        \"/StagedProducts/Hydrography/WBD/HU2/Shape/\"\n",
        "        f\"{wbd_filename}.zip\")\n",
        "    wbd_dir = et.data.get_data(url=wbd_url)                          \n",
        "                  \n",
        "    # Read desired data\n",
        "    wbd_path = os.path.join(wbd_dir, 'Shape', f'WBDHU{huc_level}.shp') \n",
        "    wbd_gdf = gpd.read_file(wbd_path, engine='pyogrio')               \n",
        "    return wbd_gdf                                                     \n",
        "\n",
        "huc_level = 12                                                         \n",
        "wbd_gdf = read_wbd_file(\n",
        "    \"WBD_08_HU2_Shape\", huc_level, cache_key=f'hu{huc_level}')         \n",
        "\n",
        "delta_gdf = (\n",
        "    wbd_gdf[wbd_gdf[f'huc{huc_level}']                                 \n",
        "    .isin(['080902030506'])]\n",
        "    .dissolve()                                                        \n",
        ")\n",
        "\n",
        "(\n",
        "    delta_gdf.to_crs(ccrs.Mercator())                                 \n",
        "    .hvplot(                                                           \n",
        "        alpha=.2, fill_color='white',                                  \n",
        "        tiles='EsriImagery', crs=ccrs.Mercator())\n",
        "    .opts(width=600, height=300)                                      \n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1de0877c",
      "metadata": {},
      "source": [
        "\n",
        "## DELTA THE MISSISSIPPI\n",
        "\n",
        "The Mississippi River Delta is where the Mississippi River meets the Gulf of Mexico in southeastern Louisiana. Covering about 3 million acres, it is one of the largest coastal wetland areas in the U.S., containing 37% of the nation's estuarine marshes. As the 7th largest river delta in the world, it drains 41% of the contiguous U.S. into the Gulf at an average rate of 470,000 cubic feet per second. \n",
        "\n",
        "The Mississippi Delta has multiple definitions, including a political one encompassing counties in several states and a natural one referring to the alluvial valley formed by sediment deposition after the Ice Age. The modern delta, built over the last 5,000 years, consists of multiple deltaic lobes, with the most recent being the \"bird’s foot\" delta near New Orleans. Ecologically, the Delta is a crucial habitat supporting wetlands, hardwood forests, migratory birds, and diverse aquatic species. Human influence on the Delta has ranged from early Indigenous agricultural practices to massive engineering projects like levees and diversions that have reshaped the landscape. While the Delta remains a major agricultural and industrial hub, environmental concerns such as water pollution and wetland loss have led to conservation and cleanup efforts.\n",
        "\n",
        "Using data from the Mississippi Delta, we will learn how to use clustering techniques on delta geospatial data, such as:\n",
        "1. introduce us to the world of clustering.\n",
        "2. how to prepare the data, datasets for analysis.\n",
        "3. implementation of the k-means algorithm and hierarchical clustering, which offers methods to evaluate the efficiency of them.\n",
        "\n",
        "Reference:<br>\n",
        "[Mississipi River Delta](https://en.wikipedia.org/wiki/Mississippi_River_Delta)<br>\n",
        "[Natural Enviroment: The Delta and Its Resources](https://www.nps.gov/locations/lowermsdeltaregion/the-natural-environment-the-delta-and-its-resources.htm)<br>\n",
        "[Mississipi Delta from space](https://es.wikipedia.org/wiki/Delta_del_r%C3%ADo_Misisipi#/media/Archivo:Mississippi_delta_from_space.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e779d07a-9ebd-4d84-af53-3fd1c211606c",
      "metadata": {},
      "source": [
        "## LAND COVER CLASSIFICATION AT THE MISSISSIPPI DELTA"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bcd6080",
      "metadata": {},
      "source": [
        "According to the USGS Open-File Report 2009-1280 delta land covers include water areas, which include rivers, lakes, estuaries and the ocean, as well as developed areas, where urban infrastructure, roads and human facilities are located. There are also mechanically disturbed lands, which correspond to areas affected by deforestation or construction activities, as well as areas dedicated to mining, where subsoil materials are extracted.\n",
        "\n",
        "Barren areas are composed of arid lands with less than 10% vegetation cover, while forests include areas with more than 10% tree density. There are also extensions of grasslands and scrublands, characterized by the presence of scattered grasses and shrubs. Agriculture is one of the predominant land covers in the region, with land used for crops, pastures, orchards and vineyards.\n",
        "\n",
        "Wetlands represent a significant part of the landscape, with highly water-saturated soils and specific vegetation. In addition, there are non-mechanically disturbed lands, which have been affected by natural phenomena such as fires or floods. Finally, although to a lesser extent, there are areas covered by ice and snow, such as glaciers or areas with permanent snow accumulations.\n",
        "\n",
        "Throughout the study period (1973-2000), the most notable changes include the conversion of approximately 4,368 km² of wetlands into wetland bodies.\n",
        "\n",
        "Reference:\n",
        "[Land-Cover Change in the Lower Mississippi Valley, 1973-2000](chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://pubs.usgs.gov/of/2009/1280/pdf/of2009-1280.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9903174-b87c-4026-8564-bd3e62dbfd14",
      "metadata": {},
      "source": [
        "### STEP 3: MULTISPECTRAL DATA\n",
        "### Search for data\n",
        "<link rel=\"stylesheet\" type=\"text/css\" href=\"./assets/styles.css\"><div class=\"callout callout-style-default callout-titled callout-task\"><div class=\"callout-header\"><div class=\"callout-icon-container\"><i class=\"callout-icon\"></i></div><div class=\"callout-title-container flex-fill\">Try It</div></div><div class=\"callout-body-container callout-body\"><ol type=\"1\">\n",
        "<li>Log in to the <code>earthaccess</code> service using your Earthdata\n",
        "credentials:\n",
        "<code>python      earthaccess.login(persist=True)</code></li>\n",
        "<li>Modify the following sample code to search for granules of the\n",
        "HLSL30 product overlapping the watershed boundary from May to October\n",
        "2023 (there should be 76 granules):\n",
        "<code>python      results = earthaccess.search_data(          short_name=\"...\",          cloud_hosted=True,          bounding_box=tuple(gdf.total_bounds),          temporal=(\"...\", \"...\"),      )</code></li>\n",
        "</ol></div></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46ededdd",
      "metadata": {},
      "source": [
        "**BLOCK 3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5f43fbb",
      "metadata": {},
      "outputs": [],
      "source": [
        "import earthaccess\n",
        "\n",
        "# log in earthaccess\n",
        "earthaccess.login(persist=True)\n",
        "\n",
        "# Ensure that delta_gdf is in geographic coordinates (WGS 84)\n",
        "if delta_gdf.crs != \"EPSG:4326\":\n",
        "    delta_gdf = delta_gdf.to_crs(\"EPSG:4326\")\n",
        "\n",
        "# define search parameters\n",
        "short_name = \"HLSL30\"  \n",
        "bounding_box = tuple(delta_gdf.total_bounds)  \n",
        "temporal_range = (\"2023-05-01\", \"2023-10-31\")  \n",
        "\n",
        "# Execute granules search \n",
        "results = earthaccess.search_data(\n",
        "    short_name=short_name,\n",
        "    cloud_hosted=True,\n",
        "    bounding_box=bounding_box,\n",
        "    temporal=temporal_range,\n",
        ")\n",
        "\n",
        "# Show the quantity of granules found\n",
        "print(f\"Number of granules found: {len(results)}\")\n",
        "\n",
        "print(\"Bounding Box:\", delta_gdf.total_bounds)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2e7859c-156a-4357-a1cd-b38192090811",
      "metadata": {},
      "source": [
        "### Compile information about each granule\n",
        "\n",
        "I recommend building a GeoDataFrame, as this will allow you to plot the\n",
        "granules you are downloading and make sure they line up with your\n",
        "shapefile. You could also use a DataFrame, dictionary, or a custom\n",
        "object to store this information.\n",
        "\n",
        "<link rel=\"stylesheet\" type=\"text/css\" href=\"./assets/styles.css\"><div class=\"callout callout-style-default callout-titled callout-task\"><div class=\"callout-header\"><div class=\"callout-icon-container\"><i class=\"callout-icon\"></i></div><div class=\"callout-title-container flex-fill\">Try It</div></div><div class=\"callout-body-container callout-body\"><ol type=\"1\">\n",
        "<li>For each search result:\n",
        "<ol type=\"1\">\n",
        "<li>Get the following information (HINT: look at the [‘umm’] values for\n",
        "each search result):\n",
        "<ul>\n",
        "<li>granule id (UR)</li>\n",
        "<li>datetime</li>\n",
        "<li>geometry (HINT: check out the shapely.geometry.Polygon class to\n",
        "convert points to a Polygon)</li>\n",
        "</ul></li>\n",
        "<li>Open the granule files. I recomment opening one granule at a time,\n",
        "e.g. with (<code>earthaccess.open([result]</code>).</li>\n",
        "<li>For each file (band), get the following information:\n",
        "<ul>\n",
        "<li>file handler returned from <code>earthaccess.open()</code></li>\n",
        "<li>tile id</li>\n",
        "<li>band number</li>\n",
        "</ul></li>\n",
        "</ol></li>\n",
        "<li>Compile all the information you collected into a GeoDataFrame</li>\n",
        "</ol></div></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fecef56e",
      "metadata": {},
      "source": [
        "**BLOCK 4**\n",
        "\n",
        "Compile information about each scene (granule)<br>\n",
        "Each scene represents a data acquisition over a specific area at a given time. In this case, for each granule (scene), we are storing:\n",
        "\n",
        "**1.Scene Metadata**\n",
        "\n",
        "-Acquisition date (datetime)<br>\n",
        "-Granule ID (granule_id)<br>\n",
        "-Spatial location (tile_id)<br>\n",
        "-Download URLs (url)<br>\n",
        "-Scene coverage (geometry - bounding box or footprint)\n",
        "\n",
        "**2.Spectral Data**\n",
        "\n",
        "-Spectral bands (B02, B03, B04, etc.)<br>\n",
        "-Cloud mask (Fmask)<br>\n",
        "-Applied scale factor (e.g., * 0.0001 for reflectance)<br>\n",
        "-Scene cropped to watershed boundary (.rio.clip())<br>\n",
        "-Quality mask applied (.where(cloud_mask == 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19f2d3e5",
      "metadata": {},
      "source": [
        "How This Function Works\n",
        "Extracts granule metadata:\n",
        "\n",
        "Granule ID (GranuleUR)<br>\n",
        "Acquisition date (BeginningDateTime)<br>\n",
        "Spatial extent (polygon geometry)<br>\n",
        "Retrieves download links for each granule using earthaccess.open()<br>\n",
        "Uses regex to extract tile ID and band info from filenames<br>\n",
        "Stores all the information in a GeoDataFrame for easy analysis and plotting<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "961d5f92",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_earthaccess_links(results):                                       \n",
        "    url_re = re.compile(                                                  \n",
        "        r'\\.(?P<tile_id>\\w+)\\.\\d+T\\d+\\.v\\d\\.\\d\\.(?P<band>[A-Za-z0-9]+)\\.tif'\n",
        "    )\n",
        "\n",
        "    # Loop through each granule\n",
        "    link_rows = []                                                       \n",
        "    for granule in tqdm(results):\n",
        "        # Get granule information\n",
        "        info_dict = granule['umm']                                        \n",
        "        granule_id = info_dict['GranuleUR']                               \n",
        "        datetime = pd.to_datetime(                                        \n",
        "            info_dict['TemporalExtent']['RangeDateTime']['BeginningDateTime']\n",
        "        )\n",
        "\n",
        "        # Extract spatial geometry (bounding polygon)\n",
        "        try:\n",
        "            points = (\n",
        "                info_dict['SpatialExtent']['HorizontalSpatialDomain']['Geometry']['GPolygons'][0]['Boundary']['Points']   \n",
        "            )\n",
        "            geometry = Polygon([(point['Longitude'], point['Latitude']) for point in points])\n",
        "        except KeyError:\n",
        "            print(f\" Warning: No geometry found for granule {granule_id}\")    \n",
        "            continue                                                          \n",
        "\n",
        "        # Get file URLs\n",
        "        files = earthaccess.open([granule])                                   \n",
        "\n",
        "        # Build metadata DataFrame\n",
        "        for file in files:\n",
        "            match = url_re.search(file.full_name)\n",
        "            if match is not None:\n",
        "                link_rows.append(\n",
        "                    dict(\n",
        "                        datetime=datetime,\n",
        "                        granule_id=granule_id,                            \n",
        "                        tile_id=match.group('tile_id'),\n",
        "                        band=match.group('band'),\n",
        "                        url=str(file),                                    \n",
        "                        geometry=geometry\n",
        "                    )\n",
        "                )\n",
        "\n",
        "    # Convert to GeoDataFrame\n",
        "    if link_rows:                                                         \n",
        "        file_df = gpd.GeoDataFrame(link_rows, crs=\"EPSG:4326\")\n",
        "        return file_df\n",
        "    else:\n",
        "        print(\"No valid granules found.\")\n",
        "        return None\n",
        "\n",
        "# Use the function with your search results\n",
        "granules_gdf = get_earthaccess_links(results)                            \n",
        "\n",
        "# Check results\n",
        "unique_granules = granules_gdf['granule_id'].nunique()\n",
        "print(f\"Unique granules processed: {unique_granules}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2afaa483",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Show the granules display on an interactive map\n",
        "granules_gdf.hvplot(geo=True, alpha=0.3, fill_color=\"red\", line_color=\"black\", tiles=\"EsriImagery\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcf59680",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Show Box coordinates\n",
        "print(\"Bounding Box:\", delta_gdf.total_bounds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6812ed50",
      "metadata": {},
      "outputs": [],
      "source": [
        "filtered_granules = granules_gdf.sjoin(delta_gdf, predicate=\"intersects\")\n",
        "print(f\"Granules intersecting the watershed: {filtered_granules['granule_id'].nunique()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c498ed42",
      "metadata": {},
      "outputs": [],
      "source": [
        "granules_gdf = granules_gdf.drop_duplicates(subset=['granule_id'])\n",
        "print(f\"Unique granules after removing duplicates: {granules_gdf['granule_id'].nunique()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3ed72ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "granules_gdf = granules_gdf.sort_values(by=\"datetime\").drop_duplicates(subset=\"granule_id\", keep=\"last\")\n",
        "print(f\"Granules after keeping latest version: {granules_gdf['granule_id'].nunique()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed023315",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final Bedugging \n",
        "# Step 1: Strict spatial filtering (fully inside watershed)\n",
        "filtered_granules = granules_gdf.sjoin(delta_gdf, predicate=\"within\")\n",
        "print(f\"Granules fully within the watershed: {filtered_granules['granule_id'].nunique()}\")\n",
        "\n",
        "# Step 2: Remove duplicate versions, keeping the latest\n",
        "granules_gdf = granules_gdf.sort_values(by=\"datetime\").drop_duplicates(subset=\"granule_id\", keep=\"last\")\n",
        "print(f\"Granules after keeping only the latest version: {granules_gdf['granule_id'].nunique()}\")\n",
        "\n",
        "# Step 3: Check unique dates\n",
        "print(\"Unique dates of granules:\", granules_gdf[\"datetime\"].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "046970ee",
      "metadata": {},
      "source": [
        "#### Expect Outcome: could some granules were only partially overlapping, the issue duplicate versions, or NASA could update the dataset with extra images. I think that is why be have 88 granules instead of 76 like the tutorial indicates."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "394fc552-7fac-4228-a60b-beb4776718e8",
      "metadata": {},
      "source": [
        "### Open, crop, and mask data\n",
        "\n",
        "This will be the most resource-intensive step. I recommend caching your\n",
        "results using the `cached` decorator or by writing your own caching\n",
        "code. I also recommend testing this step with one or two dates before\n",
        "running the full computation.\n",
        "\n",
        "This code should include at least one **function** including a\n",
        "numpy-style docstring. A good place to start would be a function for\n",
        "opening a single masked raster, applying the appropriate scale\n",
        "parameter, and cropping.\n",
        "\n",
        "<link rel=\"stylesheet\" type=\"text/css\" href=\"./assets/styles.css\"><div class=\"callout callout-style-default callout-titled callout-task\"><div class=\"callout-header\"><div class=\"callout-icon-container\"><i class=\"callout-icon\"></i></div><div class=\"callout-title-container flex-fill\">Try It</div></div><div class=\"callout-body-container callout-body\"><ol type=\"1\">\n",
        "<li>For each granule:\n",
        "<ol type=\"1\">\n",
        "<li><p>Open the Fmask band, crop, and compute a quality mask for the\n",
        "granule. You can use the following code as a starting point, making sure\n",
        "that <code>mask_bits</code> contains the quality bits you want to\n",
        "consider: ```python # Expand into a new dimension of binary bits bits =\n",
        "( np.unpackbits(da.astype(np.uint8), bitorder=‘little’)\n",
        ".reshape(da.shape + (-1,)) )</p>\n",
        "<p># Select the required bits and check if any are flagged mask =\n",
        "np.prod(bits[…, mask_bits]==0, axis=-1) ```</p></li>\n",
        "<li><p>For each band that starts with ‘B’:</p>\n",
        "<ol type=\"1\">\n",
        "<li>Open the band, crop, and apply the scale factor</li>\n",
        "<li>Name the DataArray after the band using the <code>.name</code>\n",
        "attribute</li>\n",
        "<li>Apply the cloud mask using the <code>.where()</code> method</li>\n",
        "<li>Store the DataArray in your data structure (e.g. adding a\n",
        "GeoDataFrame column with the DataArray in it. Note that you will need to\n",
        "remove the rows for unused bands)</li>\n",
        "</ol></li>\n",
        "</ol></li>\n",
        "</ol></div></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5856baf7",
      "metadata": {},
      "source": [
        "**BLOCK 5**\n",
        "\n",
        "The structured approach to process each granule, crop the Fmask band, create a cloud mask, and to each spectral band"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68e1e0da",
      "metadata": {
        "highlight": true
      },
      "outputs": [],
      "source": [
        "@cached('delta_reflectance_da_df')                        \n",
        "def compute_reflectance_da(search_results, boundary_gdf): \n",
        "    \"\"\"\n",
        "    Connect to files over VSI, crop, cloud mask, and wrangle\n",
        "    \n",
        "    Returns a single reflectance DataFrame \n",
        "    with all bands as columns and\n",
        "    centroid coordinates and datetime as the index.\n",
        "    \n",
        "    Parameters\n",
        "    ==========\n",
        "    file_df : pd.DataFrame\n",
        "        File connection and metadata (datetime, tile_id, band, and url)\n",
        "    boundary_gdf : gpd.GeoDataFrame\n",
        "        Boundary use to crop the data\n",
        "    \"\"\"\n",
        "        \n",
        "    boundary_proj_gdf = boundary_gdf.to_crs(\"EPSG:32614\")\n",
        "\n",
        "    def open_dataarray(url, boundary_proj_gdf, scale=1, masked=True): \n",
        "        # Open masked DataArray /\n",
        "        da = rxr.open_rasterio(url, masked=masked).squeeze() * scale     \n",
        "            \n",
        "        # Crop to watershed boundary \n",
        "        return da.rio.clip_box(*boundary_proj_gdf.total_bounds)\n",
        "        \n",
        "    def compute_quality_mask(da, mask_bits=[1, 2, 3]):     \n",
        "        \"\"\"Mask out low quality data by bit, that means pixel using bit flags.\"\"\"\n",
        "        # Unpack bits into a new axis \n",
        "        bits = (                                         \n",
        "            np.unpackbits(\n",
        "                da.astype(np.uint8), bitorder='little'\n",
        "            ).reshape(da.shape + (-1,))\n",
        "        )\n",
        "\n",
        "        # Select the required bits and check if any are flagged \n",
        "        mask = np.prod(bits[..., mask_bits]==0, axis=-1)\n",
        "        return mask\n",
        "\n",
        "    file_df = get_earthaccess_links(search_results)                \n",
        "    granule_da_rows= []                                            \n",
        "   \n",
        "\n",
        "    # Loop through each image \n",
        "    group_iter = file_df.groupby(['datetime', 'tile_id'])\n",
        "    for (datetime, tile_id), granule_df in tqdm(group_iter):\n",
        "        print(f'Processing granule {tile_id} {datetime}')\n",
        "              \n",
        "        # Open granule cloud cover\n",
        "        cloud_mask_url = (\n",
        "            granule_df.loc[granule_df.band=='Fmask', 'url']\n",
        "            .values[0])\n",
        "        cloud_mask_cropped_da = open_dataarray(cloud_mask_url, boundary_proj_gdf, masked=False)\n",
        "\n",
        "        # Compute cloud mask\n",
        "        cloud_mask = compute_quality_mask(cloud_mask_cropped_da)    \n",
        "\n",
        "        #Filter only spectral bands\n",
        "        band_df = granule_df[granule_df.band.str.startswith('B')]\n",
        "\n",
        "        # Loop through each spectral band/ cada banda espectral\n",
        "        for _, row in band_df.iterrows():\n",
        "            band_cropped = open_dataarray(row.url, scale=0.0001)    \n",
        "            band_cropped.name = row.band\n",
        "            row['da'] = band_cropped.where(cloud_mask)             \n",
        "            granule_da_rows.append(row.to_frame().T)                \n",
        "    \n",
        "    # Reassemble the metadata DataFrame\n",
        "    return pd.concat(rows_list, ignore_index=True)                 \n",
        "reflectance_da_df = compute_reflectance_da(results, delta_gdf)     \n",
        "                                                                   "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4090d245",
      "metadata": {},
      "source": [
        "**BLOCK 6**\n",
        "\n",
        "The goal of this funtion is process remote sensing reflectance data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfc6d9c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# visualize a processed band\n",
        "reflectance_da_df.iloc[0]['da'].hvplot.image(cmap='viridis')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3f54939-7fcf-44a6-91af-57aa50b1895c",
      "metadata": {},
      "source": [
        "### Merge and Composite Data\n",
        "\n",
        "You will notice for this watershed that: 1. The raster data for each\n",
        "date are spread across 4 granules 2. Any given image is incomplete\n",
        "because of clouds\n",
        "\n",
        "<link rel=\"stylesheet\" type=\"text/css\" href=\"./assets/styles.css\"><div class=\"callout callout-style-default callout-titled callout-task\"><div class=\"callout-header\"><div class=\"callout-icon-container\"><i class=\"callout-icon\"></i></div><div class=\"callout-title-container flex-fill\">Try It</div></div><div class=\"callout-body-container callout-body\"><ol type=\"1\">\n",
        "<li><p>For each band:</p>\n",
        "<ol type=\"1\">\n",
        "<li><p>For each date:</p>\n",
        "<ol type=\"1\">\n",
        "<li>Merge all 4 granules</li>\n",
        "<li>Mask any negative values created by interpolating from the nodata\n",
        "value of -9999 (<code>rioxarray</code> should account for this, but\n",
        "doesn’t appear to when merging. If you leave these values in they will\n",
        "create problems down the line)</li>\n",
        "</ol></li>\n",
        "<li><p>Concatenate the merged DataArrays along a new date\n",
        "dimension</p></li>\n",
        "<li><p>Take the mean in the date dimension to create a composite image\n",
        "that fills cloud gaps</p></li>\n",
        "<li><p>Add the band as a dimension, and give the DataArray a\n",
        "name</p></li>\n",
        "</ol></li>\n",
        "<li><p>Concatenate along the band dimension</p></li>\n",
        "</ol></div></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d9e5155",
      "metadata": {},
      "source": [
        "**BLOCK 7**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c4594aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "@cached('delta_reflectance_da')\n",
        "def merge_and_composite_arrays(granule_da_df):\n",
        "    \"\"\"\n",
        "    Efficiently merges and composites satellite image granules across bands and dates.\n",
        "    \"\"\"\n",
        "    da_list = []\n",
        "\n",
        "    for band, band_df in tqdm(granule_da_df.groupby('band')):\n",
        "        # Merge granules per date and mask negatives\n",
        "        merged_das = [\n",
        "            rxrmerge.merge_arrays(list(date_df.da)).where(lambda x: x > 0)\n",
        "            for _, date_df in band_df.groupby('datetime')\n",
        "        ]\n",
        "        \n",
        "        # Composite across dates using the median\n",
        "        composite_da = xr.concat(merged_das, dim='datetime').median(dim='datetime')\n",
        "\n",
        "        # Assign band metadata\n",
        "        composite_da = composite_da.assign_coords(band=int(band[1:])).expand_dims('band')\n",
        "        composite_da.name = 'reflectance'\n",
        "\n",
        "        da_list.append(composite_da)\n",
        "\n",
        "    # Concatenate all bands into a final dataset\n",
        "    return xr.concat(da_list, dim='band')\n",
        "\n",
        "reflectance_da = merge_and_composite_arrays(reflectance_da_df)\n",
        "#reflectance_da\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d4088b9-6768-4742-90c6-f6d16254db94",
      "metadata": {},
      "source": [
        "### STEP 4: K-MEANS\n",
        "\n",
        "Cluster your data by spectral signature using the k-means algorithm.\n",
        "\n",
        "<link rel=\"stylesheet\" type=\"text/css\" href=\"./assets/styles.css\"><div class=\"callout callout-style-default callout-titled callout-task\"><div class=\"callout-header\"><div class=\"callout-icon-container\"><i class=\"callout-icon\"></i></div><div class=\"callout-title-container flex-fill\">Try It</div></div><div class=\"callout-body-container callout-body\"><ol type=\"1\">\n",
        "<li>Convert your DataArray into a <strong>tidy</strong> DataFrame of\n",
        "reflectance values (hint: check out the <code>.to_dataframe()</code> and\n",
        "<code>.unstack()</code> methods)</li>\n",
        "<li>Filter out all rows with no data (all 0s or any N/A values)</li>\n",
        "<li>Fit a k-means model. You can experiment with the number of groups to\n",
        "find what works best.</li>\n",
        "</ol></div></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9967ae21",
      "metadata": {},
      "source": [
        "**BLOCK 8**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1380d9b6",
      "metadata": {
        "highlight": true
      },
      "outputs": [],
      "source": [
        "# Convert DataArray to a tidy DataFrame\n",
        "model_df = reflectance_da.to_dataframe().reset_index()\n",
        "\n",
        "# Unstack to create a feature matrix (pixels as rows, bands as columns)\n",
        "model_df = model_df.pivot(index=['y', 'x'], columns='band', values='reflectance')\n",
        "\n",
        "# Remove rows with all 0s or NaN values\n",
        "model_df = model_df[(model_df > 0).any(axis=1)].dropna()\n",
        "\n",
        "# Fit K-Means model\n",
        "n_clusters = 4  # You can adjust this number\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "\n",
        "# **Apply K-Means clustering and store results**\n",
        "model_df['clusters'] = kmeans.fit_predict(model_df)\n",
        "\n",
        "# Show sample of the clustered data\n",
        "print(model_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6417c5c9-b68e-4a7c-824a-2d1bb1aaaae5",
      "metadata": {},
      "source": [
        "### STEP 5: PLOT\n",
        "<link rel=\"stylesheet\" type=\"text/css\" href=\"./assets/styles.css\"><div class=\"callout callout-style-default callout-titled callout-task\"><div class=\"callout-header\"><div class=\"callout-icon-container\"><i class=\"callout-icon\"></i></div><div class=\"callout-title-container flex-fill\">Try It</div></div><div class=\"callout-body-container callout-body\"><p>Create a plot that shows the k-means clusters next to an RGB image of\n",
        "the area. You may need to brighten your RGB image by multiplying it by\n",
        "10. The code for reshaping and plotting the clusters is provided for you\n",
        "below, but you will have to create the RGB plot yourself!</p>\n",
        "<p>So, what is <code>.sortby(['x', 'y'])</code> doing for us? Try the\n",
        "code without it and find out.</p></div></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3520bdc3",
      "metadata": {},
      "source": [
        "**BLOCK 9**\n",
        "\n",
        "The sortby code  x, y orders the data according to spatial coordinates, that is to say that it guarantees the correct alignment between the clusters  with the RGB image,  and visualization of spatial coherence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c8a4cc9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setect R, G, B y transform to uint8\n",
        "rgb = reflectance_da.sel(band=[4, 3, 2])\n",
        "rgb_uint8 = (rgb * 255).astype(np.uint8).where(~np.isnan(rgb), 0)  # avoid NaN\n",
        "\n",
        "# restore the brigthness with control\n",
        "rgb_bright = np.clip(rgb_uint8 * 10, 0, 255)  # avoid extrem saturation\n",
        "\n",
        "# Convert clusters to xarray in correct order\n",
        "clusters_xr = model_df.clusters.to_xarray().sortby(['x', 'y'])\n",
        "\n",
        "# Visualize with `hvplot`\n",
        "plot = (\n",
        "    rgb_bright.hvplot.rgb(\n",
        "        x='x', y='y', bands='band',\n",
        "        data_aspect=1, xaxis=None, yaxis=None\n",
        "    ) +\n",
        "    clusters_xr.hvplot(cmap=\"tab10\", aspect='equal')\n",
        ")\n",
        "\n",
        "# graphics\n",
        "plot"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4efe872d-a8ab-46bd-816f-fc5194b777a5",
      "metadata": {},
      "source": [
        "## Unsupervised analysis using K-Means for class clustering\n",
        "\n",
        "### Introduction\n",
        "\n",
        "Land cover classification is a fundamental tool in environmental monitoring, as it allows for the assessment of landscape changes and their impact on ecosystems. In this analysis, we used NASA's Harmonized Landsat and Sentinel-2 (HLS) product, which provides harmonized and compatible surface reflectance (SR) data from the Landsat-8 and Sentinel-2 missions.\n",
        "\n",
        "The HLS product provides satellite imagery with Bottom of Atmosphere (BOA) surface reflectance. This means that the data has already been atmospherically corrected, removing the effects of scattering and atmospheric absorption, which enables better comparison between images from different dates or sensors.\n",
        "\n",
        "The HLS dataset integrates observations from the Operational Land Imager (OLI) onboard Landsat-8 and the Multispectral Instrument (MSI) from Sentinel-2, generating a time series of images with increased temporal frequency and spectral consistency. HLS data products can be considered the building blocks of a \"data cube\", allowing users to examine any pixel over time and analyze near-daily surface reflectance time series as if they were derived from a single sensor. This feature enhances data continuity and facilitates multitemporal land cover analysis.\n",
        "\n",
        "Additionally, the HLS product employs standardized processing methods across all images, including atmospheric correction and the Fmask algorithm for cloud masking. Surface reflectance is corrected to account for the effect of the viewing angle, ensuring that all pixels are normalized to nadir observation. This guarantees greater accuracy in the spectral interpretation of land cover.\n",
        "\n",
        "For this study, we used images acquired between May 2023 and October 2023, with a spatial resolution of 30 meters, resulting in a total of 88 scenes or granules. The data was compiled in an organized manner according to acquisition dates over the specific study area. A scale factor of 1 was assigned to all bands to mitigate size distortions (distances and areas) that increase as the distance from the central meridian grows.\n",
        "\n",
        "This study applies a supervised classification approach to identify and map different land cover classes along the Mississippi River, a key ecosystem in North America. The methodology includes image preprocessing, spectral feature extraction, and the implementation of classification algorithms to generate a detailed land cover map for the study region.\n",
        "\n",
        "The results obtained can contribute to monitoring environmental changes, managing natural resources, and making informed decisions regarding the conservation of Mississippi’s riparian ecosystems.\n",
        "\n",
        "### Metodology\n",
        "\n",
        "The analysis was conducted using the K-Means clustering algorithm to segment an image into different groups based on similar data characteristics. A total of four clusters (K=4) were selected to minimize variability within each group while maximizing the differences between them.\n",
        "\n",
        "The methodological process followed these steps:\n",
        "\n",
        "1. Data Preprocessing: The image was prepared for analysis by normalizing spectral bands or pixel values to improve clustering accuracy.\n",
        "\n",
        "2. Application of the K-Means Algorithm: The algorithm was executed with K=4, assigning each pixel to one of the four clusters based on similarity in feature space.\n",
        "\n",
        "3. Results Visualization: A segmented image was generated where each cluster was represented by a distinct color, facilitating the interpretation of homogeneous areas.\n",
        "\n",
        "4. Cluster Analysis and Interpretation: The spatial distribution of clusters was evaluated, and possible meanings were assigned based on visual appearance and geographical context.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Our data groups were identified, ensuring a classification with low internal variability and high differentiation between clusters. The K-Means algorithm, while not providing a semantic interpretation of the groups, enabled the identification of patterns in the segmented image.\n",
        "\n",
        "Visual analysis suggests the following interpretation of the clusters:\n",
        "\n",
        "Cluster 0 (Blue): Represents well-defined bodies of water.\n",
        "\n",
        "Cluster 1 (Red): Corresponds to the edges of water bodies, possibly mud or soil with a high-density cover.\n",
        "\n",
        "Cluster 2 (Pink): Areas with sparse vegetation or bare soil.\n",
        "\n",
        "Cluster 3 (Cyan): Transition zones between water and land, possibly representing wetlands or saturated soils.\n",
        "\n",
        "This analysis provides an initial approach to terrain segmentation and can be improved with additional information, such as spectral data or field validation, to enhance the interpretation of each cluster in environmental and geospatial monitoring studies."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "earth-analytics-python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
