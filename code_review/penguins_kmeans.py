# %% [markdown]
# ## Practice using k-means
# Using scikit-learn package and Palmer penguins data.
# 
# scikit-learn documentation and demos here: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html
# 
# Metric of model performance: silhouette score: the ratio between the mean difference within each cluster and mean distance to the nearest cluster (how much variability is there within a cluster vs. between clusters?)

# %% [markdown]
# ### Load packages

# %%
import os
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import rioxarray as rxr
import earthpy as et
import earthpy.plot as ep
import xarray as xr

### packages for scikit-learn
from sklearn.preprocessing import StandardScaler 
from sklearn.cluster import KMeans ### for kmeans clustering
from sklearn.decomposition import PCA ### for doing PCA
from sklearn.metrics import silhouette_score ### calculate silhouette score

### import toy data
import seaborn as sns

# %% [markdown]
# ### Load data

# %%
### load from seaborn
penguins_df = sns.load_dataset("penguins")

### see what it looks like
penguins_df

# %% [markdown]
# Looks like we have the species, island, and sex as categorical variables, and then 4 continuous variables for each individual.
# 
# We'll play around with predicting the species from the other variables. In your assignment, you'll be predicting land cover from other variables -- here, we have the advantage that we actually know what species each individual is; you won't have the actual land cover data for your data. 
# 
# BUT we have an issue: there are NaN values in the data, and k-means will not like that. Let's re-import it without the NaN values.

# %%
### load from seaborn but drop NA rows
penguins_df = sns.load_dataset("penguins").dropna()
penguins_df

# %% [markdown]
# Looks like we lost 11 rows, but we got rid of the NA values

# %% [markdown]
# ### Initial data visualization
# Play around with visualizing the data to see what it looks like

# %%
### how many species are there?
np.unique(penguins_df.species) 
### 3 total

### make a pairplot to look at continuous variables
sns.pairplot(penguins_df, hue = "species")

# %% [markdown]
# We can see that some of the variables are correlated, and some also seem to be bimodal -- we might have some success with clustering on these variables! For example, bill length and flipper length

# %% [markdown]
# ### Define a model
# We want to minimize the inertia (within-cluster variability)
# 
# Because we randomly initialize the algorithm, we might not get the absolute best clusters (k-means runs multiple times to explore different possible results). We also don't have a guarantee that the clusters generated by k-means will map on to the actual categories we're interested in.

# %%
### make a k-means model with 3 clusters (here, the hyperparameter k = 3)
k_means = KMeans(n_clusters = 3)

### fit the model to the data (specify that we want the numerical variables)
k_means.fit(penguins_df[['bill_length_mm',
                         'bill_depth_mm',
                         'flipper_length_mm',
                         'body_mass_g']])

### add the cluster labels to the dataframe
penguins_df['k_means_labels'] = k_means.labels_

### check it out
penguins_df

# %% [markdown]
# We can see the k-means labels in the last column of the dataframe now. When you shape data and add the cluster labels back in, there can be issues where the rows don't line up.

# %% [markdown]
# ### Visualize clusters

# %%
### use pairplots again
sns.pairplot(penguins_df, hue = "k_means_labels")

# %% [markdown]
# These clusters aren't lining up exactly with the species breakdown. For example, look at bill depth x body mass: cluster 2 is spanning a big gap.
# 
# Even if we didn't know the species a priori, we could still see that some of the clusters aren't getting picked out.

# %% [markdown]
# #### Try clustering on different variables
# We can limit ourselves to variables that have clearer clusters in the pairplots.

# %%
### make a k-means model with 3 clusters (here, the hyperparameter k = 3)
k_means_2 = KMeans(n_clusters = 3)

### fit the model to the data (specify which variables we want)
k_means_2.fit(penguins_df[['bill_length_mm',
                         'bill_depth_mm',
                         'flipper_length_mm']])

### add the cluster labels to the dataframe
penguins_df['k_means_labels_2'] = k_means_2.labels_

### check it out
penguins_df

# %%
### visualize
sns.pairplot(penguins_df, 
             hue = "k_means_labels_2",
             vars = [
                 'bill_length_mm',
                 'bill_depth_mm',
                 'flipper_length_mm',
                 'body_mass_g'
                 ]
                 )

# %% [markdown]
# These clusters look better than when we had all 4 variables. And even though we didn't include body mass in the model, it looks like the clusters map onto the body mass pretty well. 

# %% [markdown]
# ## Principal components analysis
# Documentation here: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html
# 
# Not required for your multispectral data, but could be useful

# %%
### run PCA
### n_components: we tell it how many components to identify (pull out the n most important components)
### if we don't set n_components, it will keep all the components
pca = PCA()

### fit the PCA to the data (specify which variables we want)
pca.fit(penguins_df[['bill_length_mm',
                         'bill_depth_mm',
                         'flipper_length_mm',
                         'body_mass_g']])

### check out the components -- this will spit out the coefficients for the linear combinations of the variables
pca.components_

# %%
### check out the variation explained by the PCAs
pca.explained_variance_ratio_

# %% [markdown]
# The first component explains 99.99% of the variance! So we probably don't even need the other components

# %% [markdown]
# #### Add PCA to the dataframe

# %%
### extract the first PCA component
b = pca.components_[0]

### make column in penguins df for component
penguins_df['component'] = (

    ### multiply the original variables by their weight in the first component
    penguins_df[['bill_length_mm']].values * b[0] 
    + penguins_df[['bill_depth_mm']].values * b[1]
    + penguins_df[['flipper_length_mm']].values * b[2]
    + penguins_df[['body_mass_g']].values * b[3]
)

### check it out
penguins_df

# %% [markdown]
# ## Cluster on the principal component

# %%
### make a k-means model with 3 clusters (here, the hyperparameter k = 3
k_means_pca = KMeans(n_clusters = 3)

### fit the model to just the first principal component
k_means_pca.fit(penguins_df[['component']])

### add the cluster labels to the dataframe
penguins_df['k_means_labels_pca'] = k_means_pca.labels_

### check it out
penguins_df

# %%
### visualize
sns.pairplot(penguins_df, 
             hue = "k_means_labels_pca",
             vars = [
                 'bill_length_mm',
                 'bill_depth_mm',
                 'flipper_length_mm',
                 'body_mass_g'
                 ]
                 )

# %% [markdown]
# Still getting some vertical stripes of clusters. Seems like in this case, reducing the number of variables in the k-means model might be better than doing the PCA

# %% [markdown]
# #### Try a different k (tune your hyperparameter)
# 
# We can try a bunch of different k values. Let's ignore the labels this time and calculate the silhouette score to see how the different k values perform

# %%

### this time we're looking at the silhouette score, so we want to accumulate it into a list
silhouette = []

### make list of k values to loop through
k_list = list(range(2, 8))

### loop through different k values
for k in k_list:

    ### make model with k clusters
    k_means = KMeans(n_clusters = k, n_init = 'auto')

    ### identify the variables to include
    model_vars = (
        penguins_df
        [['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm']])

    ### fit the model
    k_means.fit(model_vars)
        
    ### calculate silhouette score and add it to the list we initialized (along with the corresponding k value)
    silhouette.append(silhouette_score(model_vars, k_means.labels_))

### check it out
silhouette

# %%
### visualize as a scatterplot
sns.scatterplot(x = k_list, y = silhouette)

# %% [markdown]
# We can see a big jump in silhouette score from k = 2 to k = 3. Then we see diminishing returns in the silhouette score as we add additional clusters. There is value to having fewer clusters because they're more likely to be meaningful. 


